{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 852274,
          "sourceType": "datasetVersion",
          "datasetId": 450813
        }
      ],
      "dockerImageVersionId": 30407,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Thanikantijayakrishna/Adaptive-Satellite-Drone-Image-Dehazing-using-Enhanced-DCP/blob/main/duplicate_image_detection(imagehash).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "barelydedicated_airbnb_duplicate_image_detection_path = kagglehub.dataset_download('barelydedicated/airbnb-duplicate-image-detection')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jN63m2Wy40-f",
        "outputId": "02e24bff-fe74-4bec-8271-e93dbf9437c8"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/barelydedicated/airbnb-duplicate-image-detection?dataset_version_number=3...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 104M/104M [00:00<00:00, 150MB/s] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        os.path.join(dirname, filename)\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2025-04-05T02:57:46.318416Z",
          "iopub.execute_input": "2025-04-05T02:57:46.318859Z",
          "iopub.status.idle": "2025-04-05T02:57:47.088849Z",
          "shell.execute_reply.started": "2025-04-05T02:57:46.31881Z",
          "shell.execute_reply": "2025-04-05T02:57:47.087532Z"
        },
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "trusted": true,
        "id": "0xOXFQaO40-1"
      },
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": [
        "pip install imagehash"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xjc7utgn5L6e",
        "outputId": "6a851950-52e6-4e35-a930-90665478b982"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting imagehash\n",
            "  Downloading ImageHash-4.3.2-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: PyWavelets in /usr/local/lib/python3.11/dist-packages (from imagehash) (1.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from imagehash) (2.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from imagehash) (11.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from imagehash) (1.16.1)\n",
            "Downloading ImageHash-4.3.2-py2.py3-none-any.whl (296 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/296.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.7/296.7 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: imagehash\n",
            "Successfully installed imagehash-4.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing necesary libraries"
      ],
      "metadata": {
        "id": "SCJt8eqH40-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import imagehash\n",
        "import pandas\n",
        "import os\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import random"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-04-05T02:57:47.090907Z",
          "iopub.execute_input": "2025-04-05T02:57:47.091254Z",
          "iopub.status.idle": "2025-04-05T02:57:47.320333Z",
          "shell.execute_reply.started": "2025-04-05T02:57:47.091222Z",
          "shell.execute_reply": "2025-04-05T02:57:47.318946Z"
        },
        "trusted": true,
        "id": "6i8MtYZN40_B"
      },
      "outputs": [],
      "execution_count": 4
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating duplicates\n"
      ],
      "metadata": {
        "id": "NEKXQFBV40_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "80A7N6WK78gf",
        "outputId": "699b84f4-91d7-4ad7-b7ee-36cc9bfba011"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-907070862.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import random\n",
        "\n",
        "input_dir = \"/content/drive/MyDrive/Smar_Image_Detection/Airbnb Data/Test Data/living-room\"\n",
        "output_dir = \"/content/drive/MyDrive/duplicates\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "for i, filename in enumerate(os.listdir(input_dir), 1):\n",
        "    if filename.endswith(\".jpg\"):\n",
        "        img_path = os.path.join(input_dir, filename)\n",
        "        no_of_copies = random.randint(0, 4)\n",
        "        for counter in range(no_of_copies):\n",
        "            image = Image.open(img_path)\n",
        "            new_name = f\"{counter}_{filename}\"\n",
        "            image.save(os.path.join(output_dir, new_name))\n",
        "\n",
        "print(\"✅ Duplicate images saved to:\", output_dir)\n"
      ],
      "metadata": {
        "id": "OM04Cb8j8HJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Removing duplicates\n",
        "> * Using imagehash which converts the images into hash string and then futher compares.\n",
        "> * Image hashes tell whether two images look nearly identical. This is different from cryptographic hashing algorithms (like MD5, SHA-1) where tiny changes in the image give completely different hashes. In image fingerprinting, we actually want our similar inputs to have similar output hashes as well.\n",
        ">* The image hash algorithms (average, perceptual, difference, wavelet) analyse the image structure on luminance (without color information). The color hash algorithm analyses the color distribution and black & gray fractions (without position information).\n",
        "\n",
        "**Imagehash github link :** https://github.com/JohannesBuchner/imagehash"
      ],
      "metadata": {
        "id": "X-n2e9ya40_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to the directory containing the image files\n",
        "path=\"/content/drive/MyDrive/duplicates\"\n",
        "\n",
        "# Define a function to check for duplicate images in the directory\n",
        "def checkDuplication():\n",
        "    # Create an empty dictionary to store image hashes and file paths\n",
        "    images_hash={}\n",
        "    # Iterate over all files in the directory\n",
        "    for i in (os.listdir(path)):\n",
        "        # Check if the file is a JPEG image\n",
        "        if i.endswith(\".jpg\"):\n",
        "            # Get the full file path of the image\n",
        "            file_path=os.path.join(path,i)\n",
        "            # Open the image using PIL library\n",
        "            image=Image.open(file_path)\n",
        "            # Calculate the average hash of the image and convert it to a string\n",
        "            img_hash=str(imagehash.average_hash(image))\n",
        "            # Check if the hash already exists in the dictionary\n",
        "            if img_hash in images_hash:\n",
        "                # If the hash exists, print a message indicating that the image is a duplicate and delete the file\n",
        "                print(f\"There is duplication present for {i}\")\n",
        "                os.remove(file_path)\n",
        "            else:\n",
        "                # If the hash doesn't exist, add it to the dictionary with the file path as the value\n",
        "                images_hash[img_hash]=file_path\n",
        "    # Return the dictionary containing the image hashes and file paths\n",
        "    return images_hash\n",
        "\n",
        "# Call the checkDuplication function and store the result in a variable\n",
        "img_hash=checkDuplication()\n",
        "\n",
        "# Print a message indicating that duplicates have been successfully removed\n",
        "print(\"Duplicates present have been successfully removed!\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-04-05T02:58:04.947529Z",
          "iopub.execute_input": "2025-04-05T02:58:04.948569Z",
          "iopub.status.idle": "2025-04-05T02:58:05.634959Z",
          "shell.execute_reply.started": "2025-04-05T02:58:04.948493Z",
          "shell.execute_reply": "2025-04-05T02:58:05.633686Z"
        },
        "trusted": true,
        "id": "CeSNCepf40_M"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imagehash colorhash transformers tqdm --quiet\n",
        "\n",
        "import os\n",
        "import imagehash\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from colorhash import ColorHash\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import torch\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Setup\n",
        "input_dir = \"/content/drive/MyDrive/duplicates\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "# Define hashing methods\n",
        "hash_methods = {\n",
        "    'Average Hash': imagehash.average_hash,\n",
        "    'Perceptual Hash': imagehash.phash,\n",
        "    'Difference Hash': imagehash.dhash,\n",
        "    'Wavelet Hash': imagehash.whash,\n",
        "    'Color Hash': lambda img: str(ColorHash(img)),\n",
        "}\n",
        "\n",
        "# Function to compute CLIP embedding\n",
        "def get_clip_embedding(image):\n",
        "    inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n",
        "    outputs = clip_model.get_image_features(**inputs)\n",
        "    return outputs[0].detach().cpu().numpy()\n",
        "\n",
        "# Compute duplicates for traditional hashes\n",
        "def compute_duplicates(method_name, hash_func):\n",
        "    hash_dict = {}\n",
        "    duplicates_found = 0\n",
        "    total_images = 0\n",
        "\n",
        "    for filename in tqdm(os.listdir(input_dir), desc=method_name):\n",
        "        if not filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
        "            continue\n",
        "\n",
        "        filepath = os.path.join(input_dir, filename)\n",
        "        img = Image.open(filepath).convert(\"RGB\")\n",
        "        total_images += 1\n",
        "\n",
        "        try:\n",
        "            img_hash = hash_func(img)\n",
        "            img_hash = str(img_hash)\n",
        "\n",
        "            if img_hash in hash_dict:\n",
        "                duplicates_found += 1\n",
        "            else:\n",
        "                hash_dict[img_hash] = filename\n",
        "        except Exception as e:\n",
        "            print(f\"Error with {filename}: {e}\")\n",
        "\n",
        "    accuracy = (duplicates_found / total_images) * 100 if total_images else 0\n",
        "    return duplicates_found, accuracy, total_images\n",
        "\n",
        "# Compute duplicates with CLIP\n",
        "def compute_clip_duplicates(threshold=0.97):\n",
        "    embeddings = {}\n",
        "    duplicates_found = 0\n",
        "    total_images = 0\n",
        "\n",
        "    for filename in tqdm(os.listdir(input_dir), desc=\"CLIP Hash\"):\n",
        "        if not filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
        "            continue\n",
        "\n",
        "        filepath = os.path.join(input_dir, filename)\n",
        "        img = Image.open(filepath).convert(\"RGB\")\n",
        "        total_images += 1\n",
        "        emb = get_clip_embedding(img)\n",
        "\n",
        "        found = False\n",
        "        for saved_emb in embeddings.values():\n",
        "            sim = cosine_similarity([emb], [saved_emb])[0][0]\n",
        "            if sim > threshold:\n",
        "                duplicates_found += 1\n",
        "                found = True\n",
        "                break\n",
        "        if not found:\n",
        "            embeddings[filename] = emb\n",
        "\n",
        "    accuracy = (duplicates_found / total_images) * 100 if total_images else 0\n",
        "    return duplicates_found, accuracy, total_images\n",
        "\n",
        "# Evaluate all methods\n",
        "results = {}\n",
        "for name, func in hash_methods.items():\n",
        "    duplicates, accuracy, total = compute_duplicates(name, func)\n",
        "    results[name] = {\"duplicates\": duplicates, \"accuracy\": accuracy, \"total\": total}\n",
        "\n",
        "# CLIP\n",
        "clip_duplicates, clip_accuracy, clip_total = compute_clip_duplicates()\n",
        "results[\"CLIP (Crop-Resistant)\"] = {\n",
        "    \"duplicates\": clip_duplicates,\n",
        "    \"accuracy\": clip_accuracy,\n",
        "    \"total\": clip_total,\n",
        "}\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "methods = list(results.keys())\n",
        "accuracies = [results[m][\"accuracy\"] for m in methods]\n",
        "\n",
        "bars = plt.bar(methods, accuracies, color='skyblue')\n",
        "plt.title(\"Duplicate Detection Accuracy by Hashing Method\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylim(0, 100)\n",
        "\n",
        "# Annotate bars\n",
        "for bar, acc in zip(bars, accuracies):\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2.0, yval + 1, f\"{acc:.1f}%\", ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "AzimQvRi40_R"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# 🔹 LINE PLOTS FOR COMPARISON\n",
        "# ==============================\n",
        "\n",
        "# Extract values\n",
        "methods = list(results.keys())\n",
        "accuracies = [results[m][\"accuracy\"] for m in methods]\n",
        "duplicates = [results[m][\"duplicates\"] for m in methods]\n",
        "totals = [results[m][\"total\"] for m in methods]\n",
        "\n",
        "# LINE PLOT: Accuracy Comparison\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(methods, accuracies, marker='o', linestyle='-', color='blue', label='Accuracy (%)')\n",
        "plt.title(\"Accuracy Comparison of Hashing Techniques\")\n",
        "plt.xlabel(\"Hashing Method\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylim(0, 100)\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# LINE PLOT: Duplicates Found\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(methods, duplicates, marker='s', linestyle='--', color='green', label='Duplicates Found')\n",
        "plt.title(\"Duplicate Count by Hashing Method\")\n",
        "plt.xlabel(\"Hashing Method\")\n",
        "plt.ylabel(\"Number of Duplicates\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ==============================\n",
        "# 🔹 DISPLAY METRICS TABLE (OPTIONAL)\n",
        "# ==============================\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "metrics_df = pd.DataFrame({\n",
        "    \"Method\": methods,\n",
        "    \"Accuracy (%)\": accuracies,\n",
        "    \"Duplicates Found\": duplicates,\n",
        "    \"Total Images\": totals,\n",
        "})\n",
        "\n",
        "print(\"\\n📊 Comparison Metrics:\\n\")\n",
        "print(metrics_df.to_string(index=False))\n"
      ],
      "metadata": {
        "id": "r77z8HNaAegS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sUlS80ZDEu5y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}